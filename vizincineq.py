# -*- coding: utf-8 -*-
"""VizIncIneq.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lRoWwyQHfCZ2gGFDW4ymDF86Si70HkYN

#**Technical Appendix for Visualizing Income Inequality in the United States**

[Sang Truong](mailto:sangtruong_2021@depauw.edu) and [Prof. Humberto Barreto](mailto:hbarreto@depauw.edu)

Department of Economics and Management, DePauw University, Greencastle, IN 46135

## **Section 1. Introduction**

This notebook supports Barreto and Truong's 2020 paper, "Visualizing Income Inequality in the United States." It gives a detailed explanation of the transformation of the raw data to the 3D visualization. First, we import `pandas` for data manipulation and `numpy` for scientific computing. We mount Google Drive to Google Colab to read source files directly from Google Drive.
"""

import pandas as pd
import numpy as np
import json
from collections import OrderedDict
from google.colab import drive
drive.mount('/content/gdrive')
input_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/input/'
out_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/output/'

# Display 1 decimal
pd.options.display.float_format = '{:,.1f}'.format
# Show all columns
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

"""Data for analysis are from [IPUMS-CPS](https://cps.ipums.org/cps/). We download all available years in one data extract from CPS. Documentation on downloading and variables can be found on the IPUMS website [1]. For IPUMS-CPS, all selected samples are ASEC. Although IPUMS CPS has data back to 1962, geographic location (STATEFIP) is only available since 1977: "STATEFIP is comparable for 1963-1967 and 1977 onward, years in which each state and the District of Columbia were separately identified. In the remaining years, two or more states share the same code, and these groupings change over time. In 1962, 8 states cannot be separately identified. In 1968-1972, 32 states cannot be separately identified, and in 1973-1976, 38 states cannot be separately identified. In these years, up to 5 states share the same code."

We download ASEC samples from 1977 to 2019 as csv in a zip file, extract (~600MB), rename as "ipums-cps.csv" and place it in an accessible Google drive location.

For updating, note that ASEC comes out in IPUMS in October.

Years are confusing. We have ASEC CPS from 1977 to 2019. The CPI99 and HHINCOME variables in a sample year is for the previous year. So, the 1977 sample has Consumr Price Index and Household Income for 1976. Thus, we compute RHHINCOMEÂ from sample year 1977 data that is actually real household income for 1976.

Variables list: CPI99 STATEFIP HHINCOME PERNUM RELATE AGE SEX RACE HISPAN EDUC (only the first three are actually needed -- we thought we could do visualizations of subgroups, but sample sizes are too small).

"The Census Bureau fielded the CPS 2014 ASEC sample using an experimental redesign. All respondents received new health insurance questions, but 3/8ths of the total sample was randomly selected to receive the redesigned income questions. The larger portion of the sample (5/8) was given the existing questions on income. The redesign attempted to address income under-reporting, in particular, retirement, pensions, annuities, and government cash-transfer programs. More accurate income reporting in turn allows for better measurement of poverty statistics." HFLAG = 0 indicates 5/8 sample, and HFLAG = 1 indicate 3/8 sample in 2014. For the 2014 to be compatible with other data year, we select HFLAG = 0. [3]

We also create labels for states and establish starting color scheme for the visualization.
"""

# Import raw data
raw = pd.read_csv(input_path + 'ipums-cps.csv')
raw = raw[raw.YEAR >= 1977] # This line should be here regardless if you have the data before 1977 or not. It won't hurt if you don't have the data.

# Import rpp and merge with raw 
rpp = pd.read_csv('gdrive/My Drive/Colab Notebooks/USIncomeVis/input/rpp.csv')
raw = pd.merge(raw, rpp, how='outer', on = ['YEAR', 'STATEFIP'])

# Select data with HFLAG != 1 and then drop HFLAG
raw = raw[raw.HFLAG !=1]
raw = raw.drop(columns = ['HFLAG'])
print(raw.describe())

# States labels and color scheme
statefip = list(set(raw.STATEFIP))
state_name = ['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California',
              'Colorado', 'Connecticut', 'Delaware', 'District of Columbia',
              'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana',
              'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland',
              'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri',
              'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey',
              'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio',
              'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina',
              'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia',
              'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']

colors = ['#FF0000', '#FF0A00', '#FF1400', '#FF1E00', '#FF2800', '#FF3300', '#FF3D00',
          '#FF4700', '#FF5100', '#FF5B00', '#FF6600', '#FF7000', '#FF7A00', '#FF8400',
          '#FF8E00', '#FF9900', '#FFA300', '#FFAD00', '#FFB700', '#FFC100', '#FFCC00',
          '#FFD600', '#FFE000', '#FFEA00', '#FFF400', '#FFFF00', '#F4FF00', '#EAFF00',
          '#E0FF00', '#D6FF00', '#CCFF00', '#C1FF00', '#B7FF00', '#ADFF00', '#A3FF00',
          '#99FF00', '#8EFF00', '#84FF00', '#7AFF00', '#70FF00', '#66FF00', '#5BFF00',
          '#51FF00', '#47FF00', '#3DFF00', '#32FF00', '#28FF00', '#1EFF00', '#14FF00',
          '#0AFF00', '#00FF00']
colors = pd.DataFrame(colors, columns = ['Color'], index = statefip)

"""## **Section 2. adjustIncome() Function**

In this analysis, we employ 3 deflators to make better comparisons: 1) Consumer Price Index (CPI) to create real dollar values of household income over time (RHHINCOME), 2) Household size (HHSIZE) to adjust for the number of people in a household (ERHHINCOME), and 3) State price levels.

1) Because of inflation, to compare incomes over time, we need to convert nominal dollar values of HHINCOME to real values. We adjust incomes so they are all in 2018 dollars. 

2) We want to compare household incomes, but a household with two people with a total income of $100,000 is better off than a six-person household with the same income. We could just divide by the number of people in the household, but that ignores the fact that some expenses don't scale up linearly (a two-bedroom apartment is not double the rent of a one-bedroom and two people could share a car, for example). An *equivalence scale* adjusts household size. There are several options available. We tried the [*OECD equivalence scale*](http://www.oecd.org/els/soc/OECD-Note-EquivalenceScales.pdf) and the *square root equivalence scale*, as explained below.

3) Some states (e.g., California) are more expensive to live in than pther states (e.g., Alabama) so we can adjust household income by dividing by a state price index to make a better comparison.

Income adjustment can be done via `adjustIncome()` function. Below is its syntax and default parameters.

```
DataFrame adjustIncome(input_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/input/',
                       output_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/output/',
                       ipumcps_filename = 'ipums-cps.csv',
                       rpp_filename = 'rpp.csv')
```
"""

#@title **adjustIncome() Implementation for Developer**
from contextlib import contextmanager
import os, sys
@contextmanager
def suppress_stdout():
    with open(os.devnull, "w") as devnull:
        old_stdout = sys.stdout
        sys.stdout = devnull
        try: yield
        finally: sys.stdout = old_stdout
        
import pandas as pd
import numpy as np
import json
from collections import OrderedDict
from google.colab import drive
with suppress_stdout(): drive.mount('/content/gdrive')

# Display 1 decimal
pd.options.display.float_format = '{:,.1f}'.format
# Show all columns
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

def adjustIncome(input_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/input/',
                 output_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/input/',
                 ipumcps_filename = 'ipums-cps.csv',
                 rpp_filename = 'rpp.csv'):
  # Import raw data
  raw = pd.read_csv(input_path + ipumcps_filename)
  raw = raw[raw.YEAR >= 1977] # This line should be here regardless if you have the data before 1977 or not. It won't hurt if you don't have the data.

  # Import rpp and merge with raw 
  rpp = pd.read_csv(input_path + rpp_filename)
  raw = pd.merge(raw, rpp, how='outer', on = ['YEAR', 'STATEFIP'])

  # Select data with HFLAG != 1 and then drop HFLAG
  raw = raw[raw.HFLAG !=1]
  raw = raw.drop(columns = ['HFLAG'])

  # Generate Real HHINCOME in 2018 dollars
  raw['RHHINCOME'] = raw['HHINCOME']*raw['CPI99']*1.507

  # Generate size
  raw['ISIZE'] = np.nan
  raw.loc[raw['PERNUM'] == 1, 'ISIZE'] = 1
  raw.loc[(raw['PERNUM'] != 1) & (raw['AGE'] > 16), 'ISIZE'] = 1
  raw.loc[(raw['PERNUM'] != 1) & (raw['AGE'] <= 16), 'ISIZE'] = 1

  # Generate household ID
  raw['HHID'] = np.nan
  length = sum(raw.loc[raw['PERNUM'] == 1, 'PERNUM'])
  raw.loc[raw['PERNUM'] == 1, 'HHID'] = np.arange(length)
  raw = raw.fillna(method='pad')

  # Generate effective size and merge with raw
  hhsize = raw.groupby(['HHID'])['ISIZE'].agg('sum').reset_index()
  raw = pd.merge(raw, hhsize, on = ['HHID'])
  raw = raw.rename(columns={'ISIZE_x': 'ISIZE', 'ISIZE_y': 'HHSIZE'})

  # Eliminate observations that has PERNUM != 1
  raw = raw[raw['PERNUM'] == 1]

  # Squared root of HHSIZE
  raw['HHSIZE'] = (raw['HHSIZE'])**(1/2)

  # Generate Real HHINCOME in 2018 dollars
  raw['ERHHINCOME'] = raw['RHHINCOME']/raw['HHSIZE']

  # Adjusting for state price using RPP
  raw['RPPRHHINCOME'] = raw['RHHINCOME']/(raw['RPP']/100)
  raw['RPPERHHINCOME'] = raw['ERHHINCOME']/(raw['RPP']/100)

  # Generate cumulative weight and percentage
  raw['CUMWTH'] = np.nan
  raw['PERCENTH'] = np.nan

  raw.to_csv(output_path + 'processed' + ipumcps_filename, index=False)
  return raw

processedRaw = adjustIncome()

"""### **2.1. Consumer Price Index**

CPI reported by IPUMS-CPS is based on 1999 dollars. We convert CPI99 to 2018 dollars by multiplying by 1.507. Remember, CPI in a given sample year is actually the CPI for the previous year.

[This web page](https://cps.ipums.org/cps/cpi99.shtml) gives more explanation.
[HHINCOME](https://cps.ipums.org/cps-action/variables/HHINCOME#description_section) reports the total money income during the **previous calendar** year of all adult household members. "The amount should equal the sum of all household members' individual incomes as recorded in the IPUMS-CPS variableÂ [INCTOT](https://cps.ipums.org/cps-action/variables/INCTOT#description_section). The persons included were those present in the household at the time of the survey. People who lived in the household during the previous year but were not still living there at the time of the survey are not included; household members who lived elsewhere during the previous year but had joined the household at the time of the survey are included."

HHINCOME includes sources of income like wages, salaries, and business income. It can be negative. Some of the component income sources (like [INCWAGE](https://cps.ipums.org/cps-action/variables/INCWAGE#description_section) have "disclosure avoidance measures" for individuals with high incomes. Our income range is from the 5th to the 95th percentile so we avoid negative values and problems with correctly measuring extremely high incomes.
"""

# Generate Real HHINCOME in 2018 dollars
raw['RHHINCOME'] = raw['HHINCOME']*raw['CPI99']*1.507
print(raw['RHHINCOME'].describe())

"""### **2.2. Household Size SQRT**

Since our unit of analysis is household, we initialize a new dataframe (hhsize, or household size) and a new variable (ISIZE, or individual size) to account for the different in household size. For household $j$ in the sample with $n$ members,

$$ HHSIZE_j = \sum_{i = 1}^{n} ISIZE_{n} $$

For the OECD equivalence scale, each member of the household contributes to household size depending on age, like this:
* Household head (PERNUM = 1): ISIZE = 1 unit.
* Adult (PERNUM $\neq$ 1 and AGE > 16): ISIZE = 0.7 unit.
* Child (PERNUM $\neq$ 1 and AGE $\leq$ 16): ISIZE = 0.5 unit.

For the square root equivalence scale, everyone counts the same. But the equivalized household size is the square root of the sum of the members in the household. Johnson, et al. 2005, p. 13 explain the logic of the square root scale:

"This particular scale is
given by the square root of family size and indicates that the
resources for a two-person family must be 41 percent more [sqrt(2) is roughly 1.41]
than that of a single-person family for the two families to have
an equivalent standard of living. In general, the constant
elasticity scales are given by 
$$ family size^e $$
in which *e* is the
scale elasticity. Notice that if the elasticity equals one, then
the scale equals family size; there are no assumed economies
of scale in living arrangements and the equivalent resources
are simply the per capita resources. Alternatively, if the
elasticity equals zero, then there is no adjustment for family
size, there are complete economies of scale in living and the
marginal cost of another person is zero. Our chosen elasticity
of 0.5 lies halfway between these two implausible extremes
and results in âequivalentâ consumer unit resources."

We found little difference between the two scales in this analysis. We think household income per equivalized person is better than just household income, but the specific equivalence scale is applied does not matter. [4]

To calculate household size computationally efficiently (i.e., to compute household size without having to iterate through nearly 8 million lines of data), we create a new variable called HHID (household ID) by assigning a unique number to each household. With HHID, we can group each household easily.

We then compute the household size for each household ID by taking the sum of all individual sizes in the household. After that, we merge HHSIZE with the raw dataset and drop PERNUM since we no longer need it. 

Now we can adjust RHHINCOME by HHSIZE to get *equivalized real household income*, ERHHINCOME. 

This is real household income per equivalent person.
"""

# Generate size
raw['ISIZE'] = np.nan
raw.loc[raw['PERNUM'] == 1, 'ISIZE'] = 1
raw.loc[(raw['PERNUM'] != 1) & (raw['AGE'] > 16), 'ISIZE'] = 1
raw.loc[(raw['PERNUM'] != 1) & (raw['AGE'] <= 16), 'ISIZE'] = 1

# Generate household ID
raw['HHID'] = np.nan
length = sum(raw.loc[raw['PERNUM'] == 1, 'PERNUM'])
raw.loc[raw['PERNUM'] == 1, 'HHID'] = np.arange(length)
raw = raw.fillna(method='pad')

# Generate effective size and merge with raw
hhsize = raw.groupby(['HHID'])['ISIZE'].agg('sum').reset_index()
raw = pd.merge(raw, hhsize, on = ['HHID'])
raw = raw.rename(columns={'ISIZE_x': 'ISIZE', 'ISIZE_y': 'HHSIZE'})

# Eliminate observations that has PERNUM != 1
raw = raw[raw['PERNUM'] == 1]

# Squared root of HHSIZE
raw['HHSIZE'] = (raw['HHSIZE'])**(1/2)

# Generate Real HHINCOME in 2018 dollars
raw['ERHHINCOME'] = raw['RHHINCOME']/raw['HHSIZE']

print(raw[['ISIZE', 'PERNUM', 'AGE', 'HHID', 'ERHHINCOME', 'HHSIZE']].head(5))
print(raw[['ISIZE', 'PERNUM', 'AGE', 'HHID', 'ERHHINCOME', 'HHSIZE']].describe())

"""### **2.3. State price**

Prices in different states vary widely. According to the BEA, in 2018, for example, Alabamaâs state price level for all items is 86.4. California was a third higher at 115.4. Thus, adjusting for price differences across states matters. We use the [BEA's Regional Price](https://www.bea.gov/data/prices-inflation/regional-price-parities-state-and-metro-area) Parities data from 2008 (the earliest year) to 2018 to adjust RHHINCOME and ERHHINCOME.
"""

# Adjusting for state price using RPP
raw['RPPRHHINCOME'] = raw['RHHINCOME']/(raw['RPP']/100)
raw['RPPERHHINCOME'] = raw['ERHHINCOME']/(raw['RPP']/100)

# Generate cumulative weight and percentage
raw['CUMWTH'] = np.nan
raw['PERCENTH'] = np.nan

print(raw[['ERHHINCOME', 'RHHINCOME', 'RPPRHHINCOME', 'RPPERHHINCOME', 'RPP', 'CUMWTH', 'PERCENTH']].head(5))
raw.to_csv(input_path + 'processedRaw.csv', index=False)

"""## **Section 3. getIncomeVis() Function**

After the 1977 demonstration, we use `getIncomeVis()` function to process data for samples from `year_start` to `year_end`, which yields HHINCOME from `year_start-1` to `year_end-1`. Following is syntax of `getIncomeVis()` function with its default parameters:

```
(List) getIncomeVis(incomeType = 'RHHINCOME',
                         k = 'decile',
                         year_start = 1977,
                         year_end = 2019,

                         supressSort = False,
                         stateOrder = pd.DataFrame(),
                         supressPop = False,
                         normPop = DataFrame(),

                         

                         input_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/input/' + 'processedRaw.csv', 
                         output_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/output/',
                         yearCSV = False,
                         toState = True,
                         stateCSV = False,
                         provide_colorFrame = False,
                         colorFrame = pd.DataFrame(),
                         returnColor = False)
```

*   `incomeType(String)`: either `'HHINCOME'`, `'RHHINCOME'`, `'ERHHINCOME'`, `'RPPRHHINCOME'`, or `'RPPERHHINCOME'`.
*   `k(String)`: either `'decile'` or `'percentile'`. You must have `decile` and `percentile` folders in output directory to store output.
*   `year_start(int)`: starting year.
*   `year_end(int)`: ending year (unlike conventional index in Python, this year will also be included).
*   `input_path(String)`: file path of `processedRaw.csv` file.
*   `output_path(String)`: directory to output result.
*   `toState(bool)`: if `True`, output State graphs for the specified range.
*   `provide_colorFrame(bool)`: whether or not to use a precreated color frame.
*   `colorFrame(Pandas DataFrame)`: will be ignore if `provide_colorFrame = False`.
*   `returnColor(bool)`: if `True`, will only process year_start and return color frame of that year. 

*   `return: (pop, normpop, colorFrame)`:
"""

#@title **getIncomeVis() Implementation for Developer**
from contextlib import contextmanager
import os, sys
@contextmanager
def suppress_stdout():
    with open(os.devnull, "w") as devnull:
        old_stdout = sys.stdout
        sys.stdout = devnull
        try: yield
        finally: sys.stdout = old_stdout

import pandas as pd
import numpy as np
import json
from collections import OrderedDict
from google.colab import drive
with suppress_stdout(): drive.mount('/content/gdrive')

def getIncomeVis(incomeType = 'RHHINCOME',
                 k = 'decile', 
                 year_start = 1977,
                 year_end = 2019, 
                 input_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/input/' + 'processedipums-cps.csv',
                 output_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/output/',  
                 toState = False,
                 provide_colorFrame = False, 
                 colorFrame = pd.DataFrame(), 
                 returnColor = False
                 ):
  
  # Display 1 decimal
  pd.options.display.float_format = '{:,.1f}'.format
  
  # Show all columns
  pd.set_option('display.max_columns', None)
  pd.set_option('display.max_rows', None)

  # Get preprocessed raw file
  raw = pd.read_csv(input_path)

  # Generate statefip
  statefip = list(set(raw.STATEFIP))

  allPop = pd.DataFrame(columns = np.arange(year_start, year_end+1,1),index=statefip)
  allNormPop = pd.DataFrame(columns = np.arange(year_start, year_end+1,1),index=statefip)

  for year in range(year_start, year_end+1):
    # Generate year_df dataframe
    year_df = raw[raw.YEAR == year]

    # Labels 
    label_list = ['']

    state_name = ['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California',
                  'Colorado', 'Connecticut', 'Delaware', 'District of Columbia',
                  'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana',
                  'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland',
                  'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri',
                  'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey',
                  'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio',
                  'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina',
                  'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia',
                  'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']

    colors = ['#FF0000', '#FF0A00', '#FF1400', '#FF1E00', '#FF2800', '#FF3300', '#FF3D00',
              '#FF4700', '#FF5100', '#FF5B00', '#FF6600', '#FF7000', '#FF7A00', '#FF8400',
              '#FF8E00', '#FF9900', '#FFA300', '#FFAD00', '#FFB700', '#FFC100', '#FFCC00',
              '#FFD600', '#FFE000', '#FFEA00', '#FFF400', '#FFFF00', '#F4FF00', '#EAFF00',
              '#E0FF00', '#D6FF00', '#CCFF00', '#C1FF00', '#B7FF00', '#ADFF00', '#A3FF00',
              '#99FF00', '#8EFF00', '#84FF00', '#7AFF00', '#70FF00', '#66FF00', '#5BFF00',
              '#51FF00', '#47FF00', '#3DFF00', '#32FF00', '#28FF00', '#1EFF00', '#14FF00',
              '#0AFF00', '#00FF00']

    # Create state_Name and state_label dataframes
    notLabelList = list(set(state_name) - set(label_list))
    notLabelDict = dict.fromkeys(notLabelList , '')
    state_name = pd.DataFrame(data = state_name, index = statefip, columns = ['State'])
    state_label = state_name.replace(to_replace = notLabelDict)
    state_label = state_label.rename(columns={'State': 'Label'})

    # Create decile and percentile arrays
    decile = np.arange(0.05, 1.05, 0.1) # 10
    decile = np.insert(arr = decile, obj = 5, values = 0.5) # 11
    
    percentile = np.arange(0.02, 1, 0.01) # 98

	  # Create decile and percentile name
    decileName = ['5p', '15p', '25p', '35p', '45p', '50p', '55p', '65p', '75p', '85p', '95p']
    percentileName = ['2p', '3p', '4p', '5p', '6p', '7p', '8p', '9p', '10p', '11p',
                  		'12p', '13p', '14p', '15p', '16p', '17p', '18p', '19p', '20p',
                		  '21p', '22p', '23p', '24p', '25p', '26p', '27p', '28p', '29p',
                  		'30p', '31p', '32p', '33p', '34p', '35p', '36p', '37p', '38p',
                  		'39p', '40p', '41p', '42p', '43p', '44p', '45p', '46p', '47p',
                  		'48p', '49p', '50p', '51p', '52p', '53p', '54p', '55p', '56p',
                  		'57p', '58p', '59p', '60p', '61p', '62p', '63p', '64p', '65p',
                  		'66p', '67p', '68p', '69p', '70p', '71p', '72p', '73p', '74p',
                  		'75p', '76p', '77p', '78p', '79p', '80p', '81p', '82p', '83p',
                  		'84p', '85p', '86p', '87p', '88p', '89p', '90p', '91p', '92p',
                  		'93p', '94p', '95p', '96p', '97p', '98p', '99p']
                  		
    if (k == 'decile'):
      k_ile = decile
      kName = decileName
    elif (k == 'percentile'):
      k_ile = percentile
      kName = percentileName
    else: print("Invalid k")

    # Generate result grid, decile-column
    result = pd.DataFrame(data = None, index = kName, columns = statefip)

    # Iterate through each state
    c = 0
    for i in statefip:
      # Generate state dataframe
      state_df = year_df[year_df.STATEFIP == i]
      state_df = state_df.reset_index(drop = True)

      # Sort state dataframe by RHHINCOME
      state_df = state_df.sort_values(incomeType)
    
      # Calculate cumulated weight and Percentage
      state_df.CUMWTH = state_df.ASECWTH.cumsum()
      state_df.PERCENTH = state_df.CUMWTH/(state_df.ASECWTH.sum())

      # Calculate decile
      r = 0
      for _k_ile in k_ile:
        result.iloc[r,c] = state_df.loc[state_df['PERCENTH'] <= _k_ile, incomeType].max()
        r = r + 1
      c = c + 1

    # Transpose result table: column-decile
    result = result.T

    # Sort the result by median
    result = result.sort_values(by = ['50p'], ascending = True)

    # Output csv file if csv_output is True
    if (toState): result.to_csv(output_path + k + '/' + 'YEAR' + str(year-1) + '_' + incomeType + '.csv', index = True)
    # if (year == 1977):
    #   print(result)
    #   print("========================")

    colors = ['#FF0000', '#FF0A00', '#FF1400', '#FF1E00', '#FF2800', '#FF3300', '#FF3D00',
              '#FF4700', '#FF5100', '#FF5B00', '#FF6600', '#FF7000', '#FF7A00', '#FF8400',
              '#FF8E00', '#FF9900', '#FFA300', '#FFAD00', '#FFB700', '#FFC100', '#FFCC00',
              '#FFD600', '#FFE000', '#FFEA00', '#FFF400', '#FFFF00', '#F4FF00', '#EAFF00',
              '#E0FF00', '#D6FF00', '#CCFF00', '#C1FF00', '#B7FF00', '#ADFF00', '#A3FF00',
              '#99FF00', '#8EFF00', '#84FF00', '#7AFF00', '#70FF00', '#66FF00', '#5BFF00',
              '#51FF00', '#47FF00', '#3DFF00', '#32FF00', '#28FF00', '#1EFF00', '#14FF00',
              '#0AFF00', '#00FF00']
    colors = pd.DataFrame(colors, columns = ['Color'], index = statefip)
    
    if(not provide_colorFrame): colorFrame = pd.DataFrame(data = list(colors.Color), index = result.index, columns=['Color'])
    result = pd.concat([state_name, result, state_label, colorFrame], axis = 1)

    if (returnColor): return colorFrame

    # Compute state population and normalized state population
    pop = pd.DataFrame(index = statefip)
    pop['POP'] = year_df.groupby(['STATEFIP'])['ASECWT'].agg('sum')
    pop['NORMPOP'] = round(pop['POP']/(np.percentile(pop['POP'], 10)))

    allPop[year] = pop[POP]
    allNormPop[year] = pop['NORMPOP']

    # Replicate each state's dataline with its respective replication number
    for i in statefip:
      rep = pop.loc[i,'NORMPOP'] - 1
      rep = int(rep)
      line = pd.DataFrame(result.loc[i]).T
      line.loc[i, 'Label'] = ''
      for i in range(0,rep): result = pd.concat([result, line]) 

    # Sort the result by median
    result = result.sort_values(by = ['50p', 'State'], ascending = True)

    # Add the middle property
    result.reset_index(drop = True, inplace = True)

    result['Middle'] = np.nan
    counter = 0
    for state in result.State.drop_duplicates():
      temp = result[result.State == state]
      temp_size = len(temp.index)
      middle = (temp_size // 2)
      counter = counter + middle
      result.loc[counter, 'Middle'] = 1
      counter = counter - middle + temp_size

    # Convert dataframe to JSON
    result = result.to_json(orient = 'records')
    result = json.loads(result, object_pairs_hook = OrderedDict)

    # Make JSON format readable
    result = json.dumps(result, indent = 4, sort_keys = False)

    # Save JSON file -- y-1 adjusts sample year_df to HHINCOME year_df
    with open(output_path + k + '/' + 'YEAR' + str(year-1) + '_' + incomeType + '.js', 'w') as outfile:
      outfile.write('var data =')
      outfile.write(result)

  print(allNormPop)
  
  if (toState):    
    for n in statefip:
      #Reformat the columns
      data_df = []
      index_df = [i for i in range(0, 43)]
      for year in range(year_start, year_end+1):
        df = pd.read_csv(output_path + k + '/' + 'YEAR' + str(year-1) + '_' + incomeType + '.csv', index_col = 0)
        t = df.loc[n].tolist()      
        data_df.append(t)
      if (k == 'decile'): state_df = pd.DataFrame(data_df,columns = decileName,index=index_df)
      elif (k == 'percentile'): state_df = pd.DataFrame(data_df,columns=percentileName,index=index_df)
      else: print('Invalid k')
      state_df['Year'] = [i-1 for i in range(year_start, year_end + 1)]
      state_df['Label'] = ''
      if (k == 'decile'): state_df = state_df.reindex(columns = ['Year'] + decileName + ['Label'])
      elif (k == 'percentile'): state_df = state_df.reindex(columns = ['Year'] + percentileName + ['Label'])
      else: print('Invalid k')
      # if (n == 1): print(state_df)

      # Convert dataframe to JSON
      state_df = state_df.to_json(orient = 'records')
      state_df = json.loads(state_df, object_pairs_hook = OrderedDict)

      # Make JSON format readable
      state_df = json.dumps(state_df, indent = 4, sort_keys = False)

      # Save JSON file -- y-1 adjusts sample year to HHINCOME year
      with open(output_path + k + '/' + 'STATE' + str(n) + '_' + incomeType + '.js', 'w') as outfile:
        outfile.write("var data =")
        outfile.write(state_df)

  if (toState): 
    for year in range(year_start, year_end+1):
      os.remove(output_path + k + '/' + 'YEAR' + str(year-1) + '_' + incomeType + '.csv')

# Decile
colorRHHINCOME1977 = getIncomeVis(incomeType = 'RHHINCOME', returnColor = True)
getIncomeVis(incomeType = 'RHHINCOME', provide_colorFrame = True, colorFrame = colorRHHINCOME1977, toState = True)

# colorERHHINCOME1977 = getIncomeVis(incomeType = 'ERHHINCOME', returnColor = True)
# getIncomeVis(incomeType = 'ERHHINCOME', provide_colorFrame = True, colorFrame = colorERHHINCOME1977, toState = True)

# colorRPPRHHINCOME1977 = getIncomeVis(incomeType = 'RPPRHHINCOME', returnColor = True)
# getIncomeVis(incomeType = 'RPPRHHINCOME', provide_colorFrame = True, colorFrame = colorRPPRHHINCOME1977, toState = True)

# colorRPPERHHINCOME1977 = getIncomeVis(incomeType = 'RPPERHHINCOME', returnColor = True)
# getIncomeVis(incomeType = 'RPPERHHINCOME', provide_colorFrame = True, colorFrame = colorRPPERHHINCOME1977, toState = True)

# colorHHINCOME1977 = getIncomeVis(incomeType = 'HHINCOME', returnColor = True)
# getIncomeVis(incomeType = 'HHINCOME', provide_colorFrame = True, colorFrame = colorRHHINCOME1977, toState = True)

# # Percentile
# colorRHHINCOME1977 = getIncomeVis(incomeType = 'RHHINCOME', returnColor = True, k = 'percentile')
# getIncomeVis(incomeType = 'RHHINCOME', provide_colorFrame = True, colorFrame = colorRHHINCOME1977, toState = True, k = 'percentile')

# colorERHHINCOME1977 = getIncomeVis(incomeType = 'ERHHINCOME', returnColor = True, k = 'percentile')
# getIncomeVis(incomeType = 'ERHHINCOME', provide_colorFrame = True, colorFrame = colorERHHINCOME1977, toState = True, k = 'percentile')

# colorRPPRHHINCOME1977 = getIncomeVis(incomeType = 'RPPRHHINCOME', returnColor = True, k = 'percentile')
# getIncomeVis(incomeType = 'RPPRHHINCOME', provide_colorFrame = True, colorFrame = colorRPPRHHINCOME1977, toState = True, k = 'percentile')

# colorRPPERHHINCOME1977 = getIncomeVis(incomeType = 'RPPERHHINCOME', returnColor = True, k = 'percentile')
# getIncomeVis(incomeType = 'RPPERHHINCOME', provide_colorFrame = True, colorFrame = colorRPPERHHINCOME1977, toState = True, k = 'percentile')

# colorHHINCOME1977 = getIncomeVis(incomeType = 'HHINCOME', returnColor = True, k = 'percentile')
# getIncomeVis(incomeType = 'HHINCOME', provide_colorFrame = True, colorFrame = colorHHINCOME1977, toState = True, k = 'percentile')

"""### **3.1. HHINCOME**

Our goal is to prepare the data for rendering our final graph in JavaScript format. We segment HHINCOME into percentiles from 5 to 95 in steps of 10 and also the median. The code below can also produce increments of 1 percentile,  but then amCharts (our visualizer) is extremely slow.
Since we allow the user to control the states being labeled, we do not label any of them. 
We also create CUMWTH (cummulated household weight) and PERCENT (percentile for each household) as a way to rank income of each household

For the purpose of demostration and explanation, we walk through 1977 decile below to illustrate the mechanism behind the analysis. We do so by breaking the first for loop so that we render 1 graph for 1 state at a time. After that, we generate a result grid, which have the k-iles for each state at a year.
"""

# Generate year dataframe -- can enter any year from 1977 to 2019
  y = 1977
  year = raw[raw.YEAR == y]

  # Create decile and percentile arrays
  decile = np.arange(0.05, 1.05, 0.1) # 10
  decile = np.insert(arr = decile, obj = 5, values = 0.5) # 11
  percentile = np.arange(0.02, 1, 0.01) # 98

  # Create decile and percentile name
  decileName = ['5p', '15p', '25p', '35p', '45p', '50p', '55p', '65p', '75p', '85p', '95p']
  percentileName = ['2p', '3p', '4p', '5p', '6p', '7p', '8p', '9p', '10p', '11p',
                    '12p', '13p', '14p', '15p', '16p', '17p', '18p', '19p', '20p',
                    '21p', '22p', '23p', '24p', '25p', '26p', '27p', '28p', '29p',
                    '30p', '31p', '32p', '33p', '34p', '35p', '36p', '37p', '38p',
                    '39p', '40p', '41p', '42p', '43p', '44p', '45p', '46p', '47p',
                    '48p', '49p', '50p', '51p', '52p', '53p', '54p', '55p', '56p',
                    '57p', '58p', '59p', '60p', '61p', '62p', '63p', '64p', '65p',
                    '66p', '67p', '68p', '69p', '70p', '71p', '72p', '73p', '74p',
                    '75p', '76p', '77p', '78p', '79p', '80p', '81p', '82p', '83p',
                    '84p', '85p', '86p', '87p', '88p', '89p', '90p', '91p', '92p',
                    '93p', '94p', '95p', '96p', '97p', '98p', '99p']

  # Labels 
  label_list = ['']

  # Create state_Name and state_label dataframes
  notLabelList = list(set(state_name) - set(label_list))
  notLabelDict = dict.fromkeys(notLabelList , '')
  state_name = pd.DataFrame(data = state_name, index = statefip, columns = ['State'])
  state_label = state_name.replace(to_replace = notLabelDict)
  state_label = state_label.rename(columns={'State': 'Label'})

  # Choose k: decile (11) or percentile (98)
  k = 'decile'
  if (k == 'decile'):
    k_ile = decile
    kName = decileName

  if (k == 'percentile'):
    k_ile = percentile
    kName = percentileName

  # Generate result grid, decile-column
  result = pd.DataFrame(data = None, index = kName, columns = statefip)
  print(result[[1, 2, 4]].head(5))

"""For every state, we sort the adjusted household income ascendingly. We then calculate the household cumulated weight (CUMWTH) and the household percentile (PERCENTH). For household $j$th in the RHHINCOME-sorted dataframe with $k$ households:

$$CUMWTH_j = \sum_{i=0}^{j} ASECWTH_i; PERCENTH_j = \frac{CUMWTH_j}{\sum_{i=0}^{k} ASECWTH_i}$$

To calculate RHHINCOME at each k_ile $k$ for state $s$, we select a series of RHHINCOME that has percentile under or equal to $k$ (set S):
$$S_k = \{RHHINCOME_s | PERCENTH_s \leq k \} $$

RHHINCOME at k_ile $k$ for state s is the maximum value of set $S$: 

$$ RHHINCOME_{k, s} = max(S_k) $$
"""

# Iterate through each state
  c = 0
  for i in statefip:
    # Generate state dataframe
    state = year[year.STATEFIP == i]
    state = state.reset_index(drop = True)

    # Sort state dataframe by RHHINCOME
    state = state.sort_values('RHHINCOME')
    
    # Calculate cumulated weight and Percentage
    state.CUMWTH = state.ASECWTH.cumsum()
    state.PERCENTH = state.CUMWTH/(state.ASECWTH.sum())
    
    # Calculate decile
    r = 0
    for _k_ile in k_ile:
      result.iloc[r,c] = state.loc[state['PERCENTH'] <= _k_ile, 'RHHINCOME'].max()
      r = r + 1
    c = c + 1

  # Transpose result table: column-decile
  result = result.T

  # Sort the result by median
  result = result.sort_values(by = ['50p'], ascending = True)
  colors1977 = pd.DataFrame(data = list(colors.Color), index = result.index, columns=['Color'])
  result = pd.concat([state_name, result, state_label, colors1977], axis = 1)
  print(result.head(5))

"""### **3.2. Population**

We compute an estimate of the number of households in the population (POP) and then normalize it for each state in a new dataframe (NORMPOP). For state $s$ that has $k$ households in year $y$, we could make each state's population relative to the smallest state, like this:

$$POP_s = \sum_{i=0}^{k} ASECWTH_i; NORMPOP_s = \frac{POP_s}{min(POP_s)}$$ 


However, this turns out to produce an ugly graph (with extremely thin slices for the smallest states) and takes a long time to render in a browser. To improve visibility and rendering, we normalize to the 10th percentile which makes about 1/4 of the states have a width of 1 in the graph. California (the most populous state) has a width 34 times the smallest states (in actuality it should be 68 times).

We replicate each state dataline with its relative population. Sort the result dataframe by the median to get the 3D visualization to plot the states in ascending order of household income.
"""

# Compute state population and normalized state population
  pop = pd.DataFrame(index = statefip)
  pop['POP'] = year.groupby(['STATEFIP'])['ASECWT'].agg('sum')
  pop['NORMPOP'] = round(pop['POP']/(pop['POP'].min()))

  # Normalization
  pop['NORMPOP'] = round(pop['POP']/(np.percentile(pop['POP'],10)))
  print(pop.head())
  
  # Replicate each state's dataline with its respective replication number
  for i in statefip:
    rep = pop.loc[i,'NORMPOP'] - 1
    rep = int(rep)
    line = pd.DataFrame(result.loc[i]).T
    line.loc[i, 'Label'] = ''
    for i in range(0,rep): result = pd.concat([result, line])

  # Sort the result by median
  result = result.sort_values(by = ['50p', 'State'], ascending = True)
  result.reset_index(drop = True, inplace = True)

  # Enables the user to add a label to the middle slice in the graph in AmChart
  result['Middle'] = np.nan
  counter = 0
  for state in result.State.drop_duplicates():
    temp = result[result.State == state]
    temp_size = len(temp.index)
    middle = (temp_size // 2)
    counter = counter + middle
    result.loc[counter, 'Middle'] = 1
    counter = counter - middle + temp_size
  print(result.head(5))

"""### **3.3. Output to JSON**"""

# Convert dataframe to JSON, the required input for amCharts to generate the visualization.
  result = result.to_json(orient = 'records')
  result = json.loads(result, object_pairs_hook = OrderedDict)

  # Make JSON format readable
  result = json.dumps(result, indent = 4, sort_keys = False)

  # Insert result to HTML environment
  with open(out_path + k + '/' + str(y-1) + '_d_' + 'RHHHHINCOME' + '.js', 'w') as outfile:
    outfile.write('var data =')
    outfile.write(result)

"""## **Section 4. Conclusion**

We now have four variables sorted by median for each state with population determining how many times we repeat the rows for each state:

*   RHHINCOME: real household income
*   ERHHINCOME: real household income per (equivalized) person)
*   RHHINCOMERPP: real household income adjusted for state prices
*   ERHHINCOMERPP: real household income per (equivalized) person) adjusted for state prices 

With the data prepared, we turn to amCharts to render the graph.

**References:**

[1] Sarah Flood, Miriam King, Renae Rodgers, Steven Ruggles and J. Robert Warren. Integrated Public Use Microdata Series, Current Population Survey: Version 6.0 [dataset]. Minneapolis, MN: IPUMS, 2018.
https://doi.org/10.18128/D030.V6.0

[2] https://cps.ipums.org/cps-action/variables/statefip#comparability_section 

[3] https://cps.ipums.org/cps/three_eighths.shtml).

[4] Johnson, D., Smeeding T., and Boyle Torrey, B. "Economic inequality through the prisms
of income and consumption," *Monthly Labor Review* (Apr 2005), https://www.bls.gov/opub/mlr/2005/04/art2full.pdf.

[5] Color generator: https://www.strangeplanet.fr/work/gradient-generator/index.php

[6] AmChart documentation:  https://docs.amcharts.com/3/javascriptcharts/AmGraph

[7] Jack Blundell's graph: https://jackblun.github.io/Globalinc/html/fig_1980.html

## **Section 5. IncomeVis class**
"""

#@title **IncomeVis module**
import multiprocessing
import concurrent.futures
from contextlib import contextmanager
import os, sys
@contextmanager
def suppress_stdout():
    with open(os.devnull, "w") as devnull:
        old_stdout = sys.stdout
        sys.stdout = devnull
        try: yield
        finally: sys.stdout = old_stdout
import pandas as pd
import numpy as np
import json
from collections import OrderedDict
from google.colab import drive
with suppress_stdout(): drive.mount('/content/gdrive')
import matplotlib.pyplot as plt
import IPython
from IPython.display import HTML

# Pandas display: 1 decimal and all columns
pd.options.display.float_format = '{:,.1f}'.format
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

class IncomeVis:
  def __init__(self, input_path_ipums, input_path_rpp):
    self.__raw = pd.read_csv(input_path_ipums)
    self.__rpp = pd.read_csv(input_path_rpp)
    self.__raw = self.__raw[self.__raw['YEAR'] >= 1977] # Remove incomparable data
    self.__raw = pd.merge(self.__raw, self.__rpp, how = 'outer', on = ['YEAR', 'STATEFIP'])
    self.__raw = self.__raw[self.__raw['HFLAG'] !=1]
    self.__raw = self.__raw.drop(columns = ['HFLAG'])
    self.__raw = self.__raw.drop(columns = ['SEX', 'RACE', 'HISPAN', 'EDUC', 'INCTOT', 'INCWAGE']) # Optional
    self.__statefips = list(set(self.__raw['STATEFIP']))
    self.__state_name = ['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California',
                        'Colorado', 'Connecticut', 'Delaware', 'District of Columbia',
                        'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana',
                        'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland',
                        'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri',
                        'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey',
                        'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio',
                        'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina',
                        'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia',
                        'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']
    self.label_list = ['']
    notLabelList = list(set(self.__state_name) - set(self.label_list))
    notLabelDict = dict.fromkeys(notLabelList , '')
    self.__state_name = pd.DataFrame(data = self.__state_name, index = self.__statefips, columns = ['State'])
    self.state_label = self.__state_name.replace(to_replace = notLabelDict)
    self.state_label = self.state_label.rename(columns={'State': 'Label'})

    self.__colors = ['#FF0000', '#FF0A00', '#FF1400', '#FF1E00', '#FF2800', '#FF3300', '#FF3D00',
                   '#FF4700', '#FF5100', '#FF5B00', '#FF6600', '#FF7000', '#FF7A00', '#FF8400',
                   '#FF8E00', '#FF9900', '#FFA300', '#FFAD00', '#FFB700', '#FFC100', '#FFCC00',
                   '#FFD600', '#FFE000', '#FFEA00', '#FFF400', '#FFFF00', '#F4FF00', '#EAFF00',
                   '#E0FF00', '#D6FF00', '#CCFF00', '#C1FF00', '#B7FF00', '#ADFF00', '#A3FF00',
                   '#99FF00', '#8EFF00', '#84FF00', '#7AFF00', '#70FF00', '#66FF00', '#5BFF00',
                   '#51FF00', '#47FF00', '#3DFF00', '#32FF00', '#28FF00', '#1EFF00', '#14FF00',
                   '#0AFF00', '#00FF00']
    self.__colors = pd.DataFrame(self.__colors, columns = ['Color'], index = self.__statefips)
    self.__deciles = np.arange(0.05, 1.05, 0.1) # 10
    self.__deciles = np.insert(arr = self.__deciles, obj = 5, values = 0.5) # 11  
    self.__percentiles = np.arange(0.02, 1, 0.01) # 98
    self.__decileNames = ['5p', '15p', '25p', '35p', '45p', '50p', '55p', '65p', '75p', '85p', '95p']
    self.__percentileNames = ['2p', '3p', '4p', '5p', '6p', '7p', '8p', '9p', '10p', '11p',
                           '12p', '13p', '14p', '15p', '16p', '17p', '18p', '19p', '20p',
                           '21p', '22p', '23p', '24p', '25p', '26p', '27p', '28p', '29p',
                           '30p', '31p', '32p', '33p', '34p', '35p', '36p', '37p', '38p',
                           '39p', '40p', '41p', '42p', '43p', '44p', '45p', '46p', '47p',
                           '48p', '49p', '50p', '51p', '52p', '53p', '54p', '55p', '56p',
                           '57p', '58p', '59p', '60p', '61p', '62p', '63p', '64p', '65p',
                           '66p', '67p', '68p', '69p', '70p', '71p', '72p', '73p', '74p',
                           '75p', '76p', '77p', '78p', '79p', '80p', '81p', '82p', '83p',
                           '84p', '85p', '86p', '87p', '88p', '89p', '90p', '91p', '92p',
                           '93p', '94p', '95p', '96p', '97p', '98p', '99p']
    # self.__pop = pd.DataFrame(columns = np.arange(self.__raw['YEAR'].min(), self.__raw['YEAR'].max()+1,1), index = self.__statefips)
    self.__pop = pd.DataFrame()
    for year in range(self.__raw['YEAR'].min(), self.__raw['YEAR'].max()+1):
      year_df = self.__raw[self.__raw.YEAR == year]
      self.__pop['POP_' + str(year)] = year_df.groupby(['STATEFIP'])['ASECWT'].agg('sum')
      self.__pop['UR_NORMPOP_' + str(year)] = self.__pop['POP_' + str(year)]/(np.percentile(self.__pop['POP_' + str(year)], 10))
      self.__pop['NORMPOP_' + str(year)] = round(self.__pop['UR_NORMPOP_' + str(year)])
  
  def getPop (self):
    return self.__pop
  
  def getData (self):
    return self.__raw
   
  def getSTATEFIPS (self):
    return self.__statefips
 
  def adjustIncome(self):
    # 1. RHHINCOME in 2018 dollars
    self.__raw['RHHINCOME'] = self.__raw['HHINCOME']*self.__raw['CPI99']*1.507

    # 2. ERHHINCOME:
    length = len(self.__raw[self.__raw['PERNUM'] == 1])
    self.__raw.loc[self.__raw['PERNUM'] == 1, 'HHID'] = np.arange(length)
    self.__raw = self.__raw.fillna(method='pad')
    hhsize = self.__raw.groupby(['HHID']).agg(HHSIZE=('HHID', 'count'))
    self.__raw = pd.merge(self.__raw, hhsize, on = ['HHID'])
    self.__raw = self.__raw[self.__raw['PERNUM'] == 1]
    self.__raw['HHSIZE'] = (self.__raw['HHSIZE'])**(1/2)
    self.__raw['ERHHINCOME'] = self.__raw['RHHINCOME']/self.__raw['HHSIZE']
    
    # 3. RHHRHHINCOME and RHHERHHINCOME
    self.__raw['RPPRHHINCOME'] = self.__raw['RHHINCOME']/(self.__raw['RPP']/100)
    self.__raw['RPPERHHINCOME'] = self.__raw['ERHHINCOME']/(self.__raw['RPP']/100)
    
    return self.__raw

  def getIncomeVis(self,
                   incomeType = 'RHHINCOME',
                   k = 'decile',
                   year_start = 1977, year_end = 2019,
                   output_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/output/',
                   toState = False,
                   provide_colorFrame = False, colorFrame = [], returnColor = False,
                   provide_orderFrame = False, orderFrame = pd.DataFrame(), returnOrder = False,
                   AmChart = True):
    for year in range(year_start, year_end+1):
      # Generate year_df dataframe
      year_df = self.__raw[self.__raw['YEAR'] == year]

      # Decile or percentile
      if (k == 'decile'):
        kiles = self.__deciles
        kNames = self.__decileNames
      elif (k == 'percentile'):
        kiles = self.__percentiles
        kNames = self.__percentileNames
      else: raise ValueError('Illegal value of k. k can only be either decile or percentile.')

      # Generate result grid, decile-column
      result = pd.DataFrame(index = kNames, columns = self.__statefips)

      # Iterate through each state
      c = 0
      for statefip in self.__statefips:
        # Generate state dataframe
        state_df = year_df[year_df['STATEFIP'] == statefip]
        state_df = state_df.reset_index(drop = True)

        # Sort state dataframe by RHHINCOME
        state_df = state_df.sort_values(incomeType)

        # Calculate cumulated weight and Percentage
        state_df['CUMWTH'] = state_df['ASECWTH'].cumsum()
        state_df['PERCENTH'] = state_df['CUMWTH']/(state_df['ASECWTH'].sum())

        # Calculate decile
        r = 0
        for kile in kiles:
          result.iloc[r,c] = state_df.loc[state_df['PERCENTH'] <= kile, incomeType].max()
          r = r + 1
        c = c + 1

      # Transpose result table: column-decile
      result = result.T

      if (not provide_orderFrame):
        sorted_result = result.sort_values(by = ['50p'], ascending = True)
        orderFrame = sorted_result.index
      if (returnOrder): return orderFrame

      # Output csv file for toState
      if (toState): result.to_csv(output_path + k + '/State/' + 'YEAR' + str(year-1) + '_' + incomeType + '.csv', index = True)

      # Base color
      if(not provide_colorFrame): colorFrame = pd.DataFrame(data = list(self.__colors.Color), index = orderFrame, columns=['Color'])
      if (returnColor): return colorFrame
      result = pd.concat([self.__state_name, result, self.state_label, colorFrame], axis = 1)

      # Amchart vs. Matplotlib
      if (AmChart):
        # Replicate each state's dataline with its respective replication number
        for statefip in self.__statefips:
          rep = self.__pop.loc[statefip, 'NORMPOP_' + str(year)] - 1
          rep = int(rep)
          line = pd.DataFrame(result.loc[statefip]).T
          line.loc[statefip, 'Label'] = ''
          for _ in range(0, rep): result = pd.concat([result, line])
        result.reset_index(drop = False, inplace = True)
        result.rename_axis('ID', inplace = True)
        result.rename(columns={"index": "STATEFIP"}, inplace = True)
        result.set_index('STATEFIP', append=True, inplace=True)
        result = result.groupby(['STATEFIP', 'ID']).sum() # Sum has no effect since all key combinations are unique
        result = result.reindex(orderFrame, level = 'STATEFIP')

        # Add the middle property
        result.reset_index(drop = True, inplace = True)
        result['Middle'] = np.nan
        counter = 0
        for state in result.State.drop_duplicates():
          temp = result[result.State == state]
          temp_size = len(temp.index)
          middle = (temp_size // 2)
          counter = counter + middle
          result.loc[counter, 'Middle'] = 1
          counter = counter - middle + temp_size
        
        # if (year == 1977): print(result)
        
        # Convert dataframe to JSON
        result = result.to_json(orient = 'records')
        result = json.loads(result, object_pairs_hook = OrderedDict)

        # Make JSON format readable
        result = json.dumps(result, indent = 4, sort_keys = False)

        # Save JSON file -- y-1 adjusts sample year_df to HHINCOME year_df
        with open(output_path + k + '/Year/AmChart/JS/' + 'YEAR' + str(year-1) + '_' + incomeType + '.js', 'w') as outfile:
          # outfile.write('var data =')
          outfile.write(result)
      else:
        result = pd.merge(result, self.__pop['UR_NORMPOP_' + str(year)], left_index=True, right_index=True)
        result = result.reindex(index = orderFrame)
        result.to_csv(output_path + k + '/Year/Matplotlib/' + 'YEAR' + str(year-1) + '_' + incomeType + '.csv', index = True)
    
    if (toState):    
      for statefip in self.__statefips:
        #Reformat the columns
        data_df = []
        index_df = [i for i in range(0, 43)]
        for year in range(year_start, year_end+1):
          df = pd.read_csv(output_path + k + '/State/' + 'YEAR' + str(year-1) + '_' + incomeType + '.csv', index_col = 0)
          data_df.append(df.loc[statefip].tolist())
        if (k == 'decile'): state_df = pd.DataFrame(data_df,columns = decileName,index=index_df)
        elif (k == 'percentile'): state_df = pd.DataFrame(data_df,columns=percentileName,index=index_df)
        else: raise ValueError('Illegal value of k. k can only be either decile or percentile.')
        state_df['Year'] = [i-1 for i in range(year_start, year_end + 1)]
        state_df['Label'] = ''
        if (k == 'decile'): state_df = state_df.reindex(columns = ['Year'] + decileName + ['Label'])
        elif (k == 'percentile'): state_df = state_df.reindex(columns = ['Year'] + percentileName + ['Label'])
        else: raise ValueError('Illegal value of k. k can only be either decile or percentile.')

        # Convert dataframe to JSON
        state_df = state_df.to_json(orient = 'records')
        state_df = json.loads(state_df, object_pairs_hook = OrderedDict)

        # Make JSON format readable
        state_df = json.dumps(state_df, indent = 4, sort_keys = False)

        # Save JSON file -- y-1 adjusts sample year to HHINCOME year
        with open(output_path + k + '/State/' + 'STATE' + str(n) + '_' + incomeType + '.js', 'w') as outfile:
          outfile.write('var data =')
          outfile.write(state_df)
          
      for year in range(year_start, year_end+1):
        os.remove(output_path + k + '/State/' + 'YEAR' + str(year-1) + '_' + incomeType + '.csv')

  def bootstrap(self, seed = 0,
                incomeType = 'RHHINCOME',
                k = 'decile',
                year = 2018,
                statefip = 1,
                n = 1000000,
                output_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/output/bootstrap/'):
    np.random.seed(seed)
    year_df = self.__raw[self.__raw['YEAR'] == year]

    # Decile or percentile
    if (k == 'decile'):
      kiles = self.__deciles
      kNames = self.__decileNames
    elif (k == 'percentile'):
      kiles = self.__percentiles
      kNames = self.__percentileNames
    else: raise ValueError('Illegal value of k. k can only be either decile or percentile.')

    # Generate result grid, decile-column
    result = pd.DataFrame(index = kNames, columns = np.arange(n))

    # Resampling n times
    for i in range(n):
      # Generate state dataframe
      state_df = year_df[year_df['STATEFIP'] == statefip]
      state_df = state_df.reset_index(drop = True) # Optional
      sample_size = len(state_df)
      
      # Resampling
      index = np.random.choice(state_df.index, sample_size)
      state_df = state_df.iloc[index, :]
      state_df = state_df.reset_index(drop = True)

      # Sort state dataframe by RHHINCOME
      state_df = state_df.sort_values(incomeType)

      # Calculate cumulated weight and Percentage
      state_df['CUMWTH'] = state_df['ASECWTH'].cumsum()
      state_df['PERCENTH'] = state_df['CUMWTH']/(state_df['ASECWTH'].sum())

      # Calculate decile
      r = 0
      for kile in kiles:
        result.iloc[r, i] = state_df.loc[state_df['PERCENTH'] <= kile, incomeType].max()
        r = r + 1

      if (i%5000 == 0): print('iteration = ' + str(i))        
    
    result = result.T
    result.to_csv(output_path + str(seed) + 'boostrap' + incomeType + k + str(year) + '_' + str(statefip) + '_' + str(n) + '.csv', index = False)
    
    return result
  
def displayAmChart(k = 'decile', toState = False, outputHTML = False,
                   input_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/output/decile/Year/AmChart/JS/YEAR1976_HHINCOME.js',
                   output_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/output/decile/Year/AmChart/HTML/YEAR1976_HHINCOME.html'):
  if k == 'decile':
    if(not toState): html1 = open('gdrive/My Drive/Colab Notebooks/USIncomeVis/input/html1_d_year.txt', 'r')
    else: html1 = open('gdrive/My Drive/Colab Notebooks/USIncomeVis/input/html1_p_state.txt', 'r')
  elif k == 'percentile':
    if (not toState): html1 = open('gdrive/My Drive/Colab Notebooks/USIncomeVis/input/html1_p_year.txt', 'r')
    else: html1 = html1 = open('gdrive/My Drive/Colab Notebooks/USIncomeVis/input/html1_p_state.txt', 'r')
  html2 = open('gdrive/My Drive/Colab Notebooks/USIncomeVis/input/html2.txt', 'r')
  
  json = open(input_path,'r')
  AmChart = html1.read() + json.read() + html2.read()
  if outputHTML:
    with open(output_path, 'w') as outfile:
      outfile.write(AmChart)
  return IPython.display.HTML(data = AmChart)

def displayDynamic(incomeType = 'RHHINCOME', year_start = 1977, year_end = 2019,
                   input_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/output/decile/Year/Matplotlib/'):
  import pandas as pd
  from matplotlib import animation, rc
  from IPython.display import HTML
  import matplotlib.ticker as ticker
  from mpl_toolkits.mplot3d.axes3d import Axes3D
  from mpl_toolkits.mplot3d import proj3d
  import matplotlib as mpl
  import numpy as np
  import matplotlib.pyplot as plt

  # Display setting
  fig = plt.figure(figsize=(15,15))
  ax = plt.axes(projection='3d')

  x_scale=3 # Scalling
  y_scale=1
  z_scale=1
  scale=np.diag([x_scale, y_scale, z_scale, 1])
  scale=scale*(1.0/scale.max())
  scale[3,3] = 0.7
  def short_proj():
    return np.dot(Axes3D.get_proj(ax), scale)
  ax.get_proj = short_proj

  plt.subplots_adjust(left=-0.25, bottom=-0.2, right=1, top=1.25) # Bounding box adjustment
  plt.close()

  def animate(year):
    #Read the data
    pop_label = "UR_NORMPOP_" + str(year+1)
    year_df = pd.read_csv(input_path + "YEAR" + str(year) + "_" + incomeType + ".csv", index_col='State')

    ax.view_init(5,-140)
    #Convert the data to suitable format for the 3D bar chart
    deciles = ['5p','15p','25p','35p','45p','50p','55p','65p','75p','85p','95p']
    label = year_df['Label'].tolist()
    for j in range(len(label)):
      if isinstance(label[j], float): label[j] = ""

    ax.clear() #Clear the vis between each frame
    ax.set_zlim(0, 400000) #Set the limit of the z axis

    #Resize and label the x axis
    ax.set_xticks([j for j in range(len(year_df[pop_label].tolist()))])
    ax.set_xticklabels(label, rotation=45)

    ax.grid(False)

    #Resize and label the y axis
    ax.set_yticks(np.arange(len(deciles)))
    ax.set_yticklabels(["" for year in range(len(deciles))])

    ax.zaxis.set_rotate_label(False)  
    ax.set_zlabel("Anual Household Income (2018$)", rotation=90, labelpad=20, fontsize='large',fontweight='bold')
    ax.set_xlabel("Poorer States                                    " + str(year) + "                                         Richer States",
                  fontweight="bold", labelpad=20, fontsize='large')

    #Draw the 3D bar chart
    for state in range(year_df.index.size):
      for decile in range(len(deciles)):
        ax.bar3d(state, decile, 0,
                year_df.loc[year_df.iloc[state].name, pop_label]*0.025, 1,
                year_df.loc[year_df.iloc[state].name, deciles[decile]],
                color=year_df.loc[year_df.iloc[state].name, 'Color'])

  #Animation features: frames - max range for year in animate function; interval - time changing between each frame
  dynamic = animation.FuncAnimation(fig, animate, frames=[year for year in range(year_start-1,year_end)], interval=500) 
  rc('animation', html='jshtml')
  return dynamic
  
def KDE(self, variable=0, lags=40, fig=None, figsize=(15,7), savefig = False, title = None, path = None):
  from statsmodels.graphics.utils import _import_mpl, create_mpl_fig
  _import_mpl()
  fig = create_mpl_fig(fig, figsize)
  plt.close()
  resid = self
  resid = (resid - np.nanmean(resid)) / np.nanstd(resid)
  resid_nonmissing = resid[~(np.isnan(resid))]
  ax = fig.add_subplot(222)
  try: ax.hist(resid_nonmissing, 60, density=True, label='Bootstrap Histogram', facecolor='#568ae6', alpha=0.3)
  except AttributeError: ax.hist(resid_nonmissing, 60, normed=True, label='Residual Histogram', facecolor='#568ae6', alpha=0.3)
  from scipy.stats import gaussian_kde, norm
  kde = gaussian_kde(resid_nonmissing, bw_method = 0.3)
  xlim = (-1.96*2, 1.96*2)
  x = np.linspace(xlim[0], xlim[1])
  ax.plot(x, kde(x), 'r--', linewidth = 2, color = '#a924b7', label='KDE')
  ax.plot(x, norm.pdf(x), 'r--', linewidth = 2, color = '#449ff0', label=r'$\mathcal{N}(0,1)$')
  ax.set_xlim(xlim)
  ax.legend()
  ax.grid(True)
  ax.set_xlabel('', fontweight ='bold', fontsize = 'x-large')
  ax.set_ylabel(' ', fontweight ='bold', fontsize = 'x-large')
  return fig

"""The US Income Visualization library has structure of a class. Syntax of default constructor are as following:

```
IncomeVis(input_path_ipums, input_path_rpp)
```
The object supports a series of functions: 

`getPop()` : return a Dataframe of population of all states for all year in the input dataset.

`getData()` : return the data (including IPUMS-CPS and RPP). 
   
`getSTATEFIPS()` : return a list of STATEFIP code.
 
`adjustIncome()`: adjust HHINCOME with 3 deflators.

```
getIncomeVis(incomeType = 'RHHINCOME', k = 'decile', year_start = 1977, year_end = 2019,
             output_path = '/home/LDAPdir/struong21/IncomeVis/yearCSV/', suppress_sort50p = True,
             toState = False, provide_colorFrame = False, colorFrame = pd.DataFrame(),
             returnColor = False, AmChart = True)
```

```
bootstrap(seed = 0, incomeType = 'RHHINCOME', k = 'decile', year = 2018,
          statefip = 1, n = 1000000, output_path = '/home/LDAPdir/struong21/IncomeVis/yearCSV/')
```

```
KDE(self, variable=0, lags=40, fig=None, figsize=(15,7), savefig = False, title = None, path = None)
```

```
def displayAmChart(k = 'decile', toState = False, outputHTML = False,
                   input_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/output/decile/Year/AmChart/JS/YEAR1976_HHINCOME.js',
                   output_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/output/decile/Year/AmChart/HTML/YEAR1976_HHINCOME.html'):
```

```
def displayDynamic(incomeType = 'RHHINCOME', year_start = 1977, year_end = 2019,
                   input_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/output/decile/Year/Matplotlib/'):
```
"""

# Create object and adjust income
myIncomeVis = IncomeVis('gdrive/My Drive/Colab Notebooks/USIncomeVis/input/ipums-cps.csv',
                        'gdrive/My Drive/Colab Notebooks/USIncomeVis/input/rpp.csv')
myIncomeVis.adjustIncome()

# Compute data for AmChart following 1977 order
colorHHINCOME1977 = myIncomeVis.getIncomeVis(incomeType = 'HHINCOME', returnColor = True, year_start = 1977)
orderHHINCOME1977 = myIncomeVis.getIncomeVis(incomeType = 'HHINCOME', returnOrder = True, year_start = 1977)
myIncomeVis.getIncomeVis(incomeType = 'HHINCOME',
                         provide_colorFrame = True, colorFrame = colorHHINCOME1977,
                         provide_orderFrame = True, orderFrame = orderHHINCOME1977)

# Compute data for Matplotlib following 1977 order
colorRHHINCOME1977 = myIncomeVis.getIncomeVis(incomeType = 'RHHINCOME', returnColor = True, year_start = 1977, AmChart= False)
orderRHHINCOME1977 = myIncomeVis.getIncomeVis(incomeType = 'RHHINCOME', returnOrder = True, year_start = 1977, AmChart= False)
myIncomeVis.getIncomeVis(incomeType = 'RHHINCOME', AmChart= False,
                         provide_colorFrame = True, colorFrame = colorRHHINCOME1977,
                         provide_orderFrame = True, orderFrame = orderRHHINCOME1977)

# Compute data for Matplotlib following 2019 order
colorRHHINCOME1977 = myIncomeVis.getIncomeVis(incomeType = 'RHHINCOME', returnColor = True, year_start = 1977, AmChart= False)
orderRHHINCOME2019 = myIncomeVis.getIncomeVis(incomeType = 'RHHINCOME', returnOrder = True, year_start = 2019, AmChart= False)
myIncomeVis.getIncomeVis(incomeType = 'RHHINCOME', AmChart= False,
                         year_start = 2019, year_end = 2019,
                         provide_colorFrame = True, colorFrame = colorRHHINCOME1977,
                         provide_orderFrame = True, orderFrame = orderRHHINCOME2019)

# Parallel bootstrap helper allows fast and large scale bootstrapping by enabling multi-cores processing
def _bootstrap(seed):
  print(str(seed) + ' is running')  
  return myIncomeVis.bootstrap(seed = seed, n = 62500, statefip = 1, year = 2009, incomeType = 'RPPERHHINCOME')

def parallel_bootstrap():
  with concurrent.futures.ProcessPoolExecutor() as executor:
    results = [executor.submit(_bootstrap, seed) for seed in  range(16)]
parallel_bootstrap()

# 1M bootstrap of RPPERHHINCOME of STATEFIP = 1 in 2008
RPPERHHINCOME2008_1_1M = pd.read_csv('gdrive/My Drive/Colab Notebooks/USIncomeVis/RPPERHHINCOME_1_2008_1M.csv')
plt.figure(figsize=(6.75,3.5))
plt.hist(RPPERHHINCOME2008_1_1M['50p'], 50, density = True, facecolor='blue', alpha=0.75)
plt.show()

# Kernel density estimation
KDE(data1977_1M['50p'])

# Display AmChart graph
displayAmChart(input_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/output/decile/Year/AmChart/JS/YEAR2018_HHINCOME.js',
               output_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/output/decile/Year/AmChart/HTML/YEAR2018_HHINCOME.html',
               outputHTML = True)

# Display dynamic
displayDynamic()

#@title **Dynamic with re-order at the end**

incomeType = 'RHHINCOME'
year_start = 1977
year_end = 2019
input_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/output/decile/Year/Matplotlib/'

import pandas as pd
from matplotlib import animation, rc
from IPython.display import HTML
import matplotlib.ticker as ticker
from mpl_toolkits.mplot3d.axes3d import Axes3D
from mpl_toolkits.mplot3d import proj3d
import matplotlib as mpl
import numpy as np
import matplotlib.pyplot as plt

# Display setting
fig = plt.figure(figsize=(15,15))
ax = plt.axes(projection='3d')

x_scale=3 # Scalling
y_scale=1
z_scale=1
scale=np.diag([x_scale, y_scale, z_scale, 1])
scale=scale*(1.0/scale.max())
scale[3,3] = 0.7
def short_proj():
  return np.dot(Axes3D.get_proj(ax), scale)
ax.get_proj = short_proj

plt.subplots_adjust(left=-0.25, bottom=-0.2, right=1, top=1.25) # Bounding box adjustment
plt.close()

def animate(year):
  #Read the data
  if year == 2019:
    year_df = pd.read_csv(input_path + "YEAR2018_RHHINCOME_2018order.csv", index_col='State')
    pop_label = "UR_NORMPOP_2019"
  else:
    year_df = pd.read_csv(input_path + "YEAR" + str(year) + "_" + incomeType + ".csv", index_col='State')
    pop_label = "UR_NORMPOP_" + str(year+1)

  ax.view_init(5,-140)
  #Convert the data to suitable format for the 3D bar chart
  deciles = ['5p','15p','25p','35p','45p','50p','55p','65p','75p','85p','95p']
  label = year_df['Label'].tolist()
  for j in range(len(label)):
    if isinstance(label[j], float): label[j] = ""

  ax.clear() #Clear the vis between each frame
  ax.set_zlim(0, 400000) #Set the limit of the z axis

  #Resize and label the x axis
  ax.set_xticks([j for j in range(len(year_df[pop_label].tolist()))])
  ax.set_xticklabels(label, rotation=45)

  ax.grid(False)

  #Resize and label the y axis
  ax.set_yticks(np.arange(len(deciles)))
  ax.set_yticklabels(["" for year in range(len(deciles))])

  ax.zaxis.set_rotate_label(False)  
  ax.set_zlabel("Anual Household Income (2018$)", rotation=90, labelpad=20, fontsize='large',fontweight='bold')
  ax.set_xlabel("Poorer States                                    " + str(year) + "                                         Richer States",
                fontweight="bold", labelpad=20, fontsize='large')

  #Draw the 3D bar chart
  for state in range(year_df.index.size):
    for decile in range(len(deciles)):
      ax.bar3d(state, decile, 0,
              year_df.loc[year_df.iloc[state].name, pop_label]*0.025, 1,
              year_df.loc[year_df.iloc[state].name, deciles[decile]],
              color=year_df.loc[year_df.iloc[state].name, 'Color'])

#Animation features: frames - max range for year in animate function; interval - time changing between each frame
dynamic = animation.FuncAnimation(fig, animate, frames=[year for year in range(year_start-1,year_end+1)], interval=500) 
rc('animation', html='jshtml')
dynamic